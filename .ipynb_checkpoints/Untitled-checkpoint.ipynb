{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exploration_rate(self, current_step):\n",
    "    return self.end + (self.start - self.end) * \\\n",
    "        math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).item() # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.0005\n",
    "discount_rate = .99\n",
    "max_steps = 500\n",
    "num_episodes = 100000\n",
    "num_steps_until_reset_target = 5000\n",
    "batch_size = 100\n",
    "\n",
    "#exploration rate values\n",
    "eps = 1\n",
    "min_eps_val = 0.01\n",
    "max_eps_val = 1\n",
    "eps_decay_rate = 0.001\n",
    "\n",
    "#initialize replay memory\n",
    "replay_memory = []\n",
    "replay_memory_size = 5000\n",
    "push_count = 0\n",
    "\n",
    "#initializes the networks\n",
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "rewards = []\n",
    "avg = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "#     if episode % 200 == 0:\n",
    "#         print(episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    current_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # env.render()\n",
    "        #updates the target network\n",
    "        if push_count % num_steps_until_reset_target == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        #Selects action via exploration or exploitation\n",
    "        if np.random.random_sample() > eps:\n",
    "            with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        #execute selected action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #store in replay memory\n",
    "        if len(replay_memory) < replay_memory_size:\n",
    "             replay_memory.append((state, action, reward, new_state, done))\n",
    "        else:\n",
    "            replay_memory[push_count % replay_memory_size] = (state, action, reward, new_state, done)\n",
    "        push_count += 1\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            \n",
    "        state_action_values = policy_net([:,0]).gather(1, [:,1])\n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "\n",
    "        current_reward += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    #decays the epsilon\n",
    "    eps = min_eps_val + (max_eps_val - min_eps_val) * np.exp(-eps_decay_rate*episode)\n",
    "    avg += current_reward\n",
    "    if episode % 1000 == 0:\n",
    "        print(avg / 1000)\n",
    "        avg = 0\n",
    "        rewards.append(current_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
